{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURPOSE: This script will be used to scrape the web for data and rewrite it with GPT-3\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import openai\n",
    "import frontmatter\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_class(url: str, class_name: str) -> str:\n",
    "    try:\n",
    "        # Send a request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find elements with the specified class\n",
    "        elements = soup.find_all(class_=class_name)\n",
    "\n",
    "        # Extract and return the text from these elements\n",
    "        text = [element.get_text(strip=True) for element in elements]\n",
    "        return text[0]\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return []\n",
    "\n",
    "def split_text(text, max_words=2900):\n",
    "    words = text.split(\" \")\n",
    "    for i in range(0, len(words), max_words):\n",
    "        yield ' '.join(words[i:i + max_words])\n",
    "\n",
    "def rewrite_text(chunk, open_ai_url, headers, is_first_chunk):\n",
    "    prompt = \"\"\"REWRITE THE FOLLOWING TEXT IN A CONFIDENT TONE. THE TEXT YOU GENERATE SHOULD BE \n",
    "     COMPLETELY DIFFERENT FROM THE ORIGINAL TEXT. \n",
    "     \n",
    "     If there is any 'proprietary language' such as 'Our Research' or 'Our Study', please remove it. \n",
    "    Make sure every sentence is rewritten.  Also, please remove any references to \n",
    "    'McKinsey' or 'Quantum Black'.\\n\"\"\"\n",
    "\n",
    "    if is_first_chunk:\n",
    "        prompt += \"\"\"\n",
    "        Add frontmatter to the top of the document with the following information,\n",
    "        - summary point is only one sentence\n",
    "        - Provide a maximum of three points \n",
    "        - the title and subTitle have quotes (i.e. \"<title>\") \n",
    "\n",
    "        Return the text starting with the frontmater (i.e. \"---\") and nothing else.\n",
    "        ---\n",
    "        title: \"<title in quotes>\"\n",
    "        subTitle: \"<subTitle in quotes>\"\n",
    "        category: <category>\n",
    "        date: <Month Year> \n",
    "        headers:\n",
    "        -  \"Cache-Control: max-age=86400\"\n",
    "        recommended: true\n",
    "\n",
    "        points:\n",
    "        - <point 1>\n",
    "        - <point 2>\n",
    "        - <point 3>\n",
    "        ---\n",
    "        \"\"\"\n",
    "\n",
    "    prompt = prompt + f\"**text**\\n{chunk}\"\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "    #print(\"Prompt: \\n\\n\\n\", prompt, ' \\n\\n\\n\\n')\n",
    "\n",
    "    response = requests.post(open_ai_url, headers=headers, data=json.dumps(data))\n",
    "    try:\n",
    "        return json.loads(response.text)['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print( json.loads( response.text ), '<<< response \\n\\n\\n' )\n",
    "\n",
    "\n",
    "\n",
    "def create_folder_and_file_from_md(text, source_url, base_path=\"../general\"):\n",
    "    # Parse the frontmatter\n",
    "    parsed = frontmatter.loads(text)\n",
    "    title = parsed.metadata.get('title', 'Untitled').replace(':', '-')\n",
    "\n",
    "    # Create a folder named after the title\n",
    "    folder_path = os.path.join(base_path, title)\n",
    "    folder_path = folder_path.replace(':', '-')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Write the content into a file within the new folder\n",
    "    file_path = os.path.join(folder_path, title + '.md')\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(frontmatter.dumps(parsed))\n",
    "\n",
    "    # save source\n",
    "    file_path = os.path.join(folder_path, 'source.txt')\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(source_url)\n",
    "\n",
    "def process_text(text, url, headers):\n",
    "    rewritten_chunks = []\n",
    "    is_first_chunk = True\n",
    "\n",
    "    for chunk in split_text(text):\n",
    "        rewritten_chunk = rewrite_text(chunk, url, headers, is_first_chunk)\n",
    "        rewritten_chunks.append(rewritten_chunk)\n",
    "        is_first_chunk = False  # Only the first chunk includes the frontmatter prompt\n",
    "        #print(f'-------------------- Completed chunk @ {datetime.now()}')\n",
    "\n",
    "    return ' '.join(rewritten_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.get('GPT_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from a given URL and class name\n",
    "\n",
    "# URL and class name\n",
    "urls = [ \n",
    "    \"https://www.mckinsey.com/industries/real-estate/our-insights/generative-ai-can-change-real-estate-but-the-industry-must-change-to-reap-the-benefits\",\n",
    "    \"https://www.mckinsey.com/capabilities/quantumblack/our-insights/winning-with-ai-is-a-state-of-mind\",\n",
    "    \"https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-data-dividend-fueling-generative-ai\",\n",
    "    \"https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/data-ethics-what-it-means-and-what-it-takes\",\n",
    "    \"https://www.mckinsey.com/capabilities/quantumblack/our-insights/demystifying-data-mesh\",\n",
    "    \"https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/how-data-can-help-tech-companies-thrive-amid-economic-uncertainty\",\n",
    "    \"https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-data-driven-enterprise-of-2025\",\n",
    "    \"https://www.mckinsey.com/capabilities/risk-and-resilience/our-insights/model-risk-management-2-point-0-evolves-to-address-continued-uncertainty-of-risk-related-events\",\n",
    "    \"https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/rewired-and-running-ahead-digital-and-ai-leaders-are-leaving-the-rest-behind\",\n",
    "    \"https://www.mckinsey.com/capabilities/growth-marketing-and-sales/our-insights/how-generative-ai-can-boost-consumer-marketing\",\n",
    "    \"https://www.mckinsey.com/capabilities/strategy-and-corporate-finance/our-insights/gen-ai-a-guide-for-cfos\",\n",
    "    \"https://www.mckinsey.com/industries/consumer-packaged-goods/our-insights/how-consumer-companies-outcompete-high-performing-operating-models\",\n",
    "    \"https://www.mckinsey.com/capabilities/quantumblack/our-insights/how-to-unlock-the-full-value-of-data-manage-it-like-a-product\",\n",
    "    \"https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/technologys-generational-moment-with-generative-ai-a-cio-and-cto-guide\",\n",
    "    \"https://www.mckinsey.com/capabilities/quantumblack/our-insights/four-essential-questions-for-boards-to-ask-about-generative-ai\",\n",
    "    \"https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier\",\n",
    "    \"https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/tech-forward/realizing-more-value-from-data-projects\"\n",
    "]\n",
    "class_name = \"mdc-o-content-body mck-u-dropcap\"\n",
    "\n",
    "# Extract and print text\n",
    "for url in urls:\n",
    "    extracted_text = extract_text_from_class(url, class_name)\n",
    "\n",
    "    # usage\n",
    "    extracted_text = extracted_text\n",
    "    open_ai_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    openai_api_key ='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' #os.environ.get('GPT_ACCESS_KEY')\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "    }\n",
    "\n",
    "    final_text = process_text(extracted_text, open_ai_url, headers)\n",
    "    create_folder_and_file_from_md(final_text, url)\n",
    "    #print(f'-------------------- Response {final_text} \\n\\n\\n')\n",
    "    print(f'-------------------- Completed {url} @ {datetime.now()} \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- OLD CODE -------------------- \n",
    "def split_text(text, max_words=3000):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), max_words):\n",
    "        yield ' '.join(words[i:i + max_words])\n",
    "\n",
    "def rewrite_text_with_openai(chunk):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci\", \n",
    "        prompt=\"Rewrite the following text in a confident tone. Remove :\\n\" + chunk,\n",
    "        max_tokens=4096  # Adjust as needed\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def process_text(text):\n",
    "    rewritten_chunks = []\n",
    "\n",
    "    for chunk in split_text(text):\n",
    "        rewritten_chunk = rewrite_text_with_openai(chunk)\n",
    "        rewritten_chunks.append(rewritten_chunk)\n",
    "\n",
    "    return ' '.join(rewritten_chunks)\n",
    "\n",
    "# Your API key for OpenAI\n",
    "openai_api_key = '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'\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Your long text\n",
    "long_text = extracted_text\n",
    "\n",
    "# Process the text\n",
    "final_text = process_text(long_text[0])\n",
    "print(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_markdown_with_frontmatter(text):\n",
    "    # Split the text at the start of frontmatter and end of frontmatter\n",
    "    frontmatter_end_index = text.find('---', 3) + 3\n",
    "    frontmatter = text[:frontmatter_end_index]\n",
    "    content = text[frontmatter_end_index:]\n",
    "\n",
    "    # Split the content into sections based on '\\n\\n'\n",
    "    sections = content.split('\\n\\n')\n",
    "\n",
    "    return frontmatter, sections\n",
    "\n",
    "frontmatter_, sections = parse_markdown_with_frontmatter(rewritten_text)\n",
    "#print(\"Frontmatter:\\n\\n\", frontmatter)\n",
    "#for section in sections:\n",
    "#    print(\"Section:\\n\\n\", section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rewrite the text via OpenAI\n",
    "openai_api_key = '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'\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": f\"\"\"\n",
    "    Rewrite the following text in a confident tone. If there is any \"proprietary \n",
    "    language such as \"Our Research\" or \"Our Study\" please remove it. Also, please\n",
    "    remove any references to \"McKinsey\" or \"Quantum Black\".\n",
    "\n",
    "Add frontmatter to the top of the document with the following information,\n",
    "where summary point is only one sentence and there is a maximum of three points.\n",
    "---\n",
    "title: <title>\n",
    "subTitle: <subTitle>\n",
    "category: <category>\n",
    "date: <Month Year> \n",
    "headers:\n",
    "  Cache-Control: max-age=86400\n",
    "recommended: true\n",
    "\n",
    "points:\n",
    "  - <point 1>\n",
    "  - <point 2>\n",
    "  - <point 3>\n",
    "---\n",
    "\n",
    "    **text**\n",
    "    {extracted_text[0]}\n",
    "    \"\"\"}]\n",
    "}\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "rewritten_text = json.loads( response.text )['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## OLD ##\n",
    "def split_text(text, max_words=2000):\n",
    "    words = text[0].split()\n",
    "    for i in range(0, len(words), max_words):\n",
    "        yield ' '.join(words[i:i + max_words])\n",
    "\n",
    "def rewrite_text(chunk, open_ai_url, headers):\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": f\"\"\"\n",
    "        Rewrite the following text in a confident tone. If there is any \"proprietary \n",
    "        language such as \"Our Research\" or \"Our Study\" please remove it. Also, please\n",
    "        remove any references to \"McKinsey\" or \"Quantum Black\".\n",
    "\n",
    "        Add frontmatter to the top of the document with the following information,\n",
    "        summary point is only one sentence and there is a maximum of three points. And, the \n",
    "        \"title\" and \"subTitle\" should be wrapped in quotes.\n",
    "        ---\n",
    "        title: <title>\n",
    "        subTitle: <subTitle>\n",
    "        category: <category>\n",
    "        date: <Month Year> \n",
    "        headers:\n",
    "        Cache-Control: max-age=86400\n",
    "        recommended: true\n",
    "\n",
    "        points:\n",
    "        - <point 1>\n",
    "        - <point 2>\n",
    "        - <point 3>\n",
    "        ---\n",
    "\n",
    "            **text**\n",
    "            {chunk}\n",
    "        \"\"\"}]\n",
    "    }\n",
    "    print(len(data['messages'][0]['content']), '<<<< len(data) \\n\\n\\n')\n",
    "    print(data, '<<<< data \\n\\n\\n')\n",
    "    print(open_ai_url, '<<<< open_ai_url \\n')\n",
    "    response = requests.post(open_ai_url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    print( json.loads( response.text ), '<<< response \\n\\n\\n' )\n",
    "    return json.loads(response.text)['choices'][0]['message']['content']\n",
    "\n",
    "def process_text(text, url, headers):\n",
    "    rewritten_chunks = []\n",
    "\n",
    "    for chunk in split_text(text):\n",
    "        rewritten_chunk = rewrite_text(chunk, url, headers)\n",
    "        rewritten_chunks.append(rewritten_chunk)\n",
    "        print(f'Completed chunk @ {datetime.now()}')\n",
    "\n",
    "    return ' '.join(rewritten_chunks)\n",
    "\n",
    "# usage\n",
    "extracted_text = extracted_text\n",
    "open_ai_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "openai_api_key = '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'\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "}\n",
    "\n",
    "final_text = process_text(extracted_text, open_ai_url, headers)\n",
    "print(final_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
